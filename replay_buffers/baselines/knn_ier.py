"""Python class for Interpolated Experience Replay with Nearest Neighbors. This
class serves as a baseline test for comparing to Gaussian Process regression.
"""
# External Python packages
import numpy as np

# Native Python packages
import time

# Ray/RLlib
_ALL_POLICIES = "__all__"

# Custom Python packages/modules
from replay_buffers.interpolated_replay.ier_base import InterpolatedReplayBuffer
from utils.visualization.visualization import pca_holdout


class KnnIER(InterpolatedReplayBuffer):
    """Implements K-Nearest Neighbors Interpolated Experience Replay (KNN-IER),
    which samples interpolated_replay batches using the nearest neighbor point of a
    given point. Serves as a baseline for LIER and BIER.

    Class for KNN-IER. Inherits from InterpolatedReplayBuffer.
    Python class for KNN Interpolated Experience Replay. Interpolated samples
    are generated by sampling states and actions, and assigning the predicted
    reward and new state/delta state to be the reward and state/delta state of
    the given sample point's nearest neighbor.

    Some notes and recommendations on this model:

    1. Since only the nearest neighbor is considered, this class automatically
    sets the number of neighbors equal to 2.

    The parameters for all types of interpolated_replay experience replay are given
    in ier_base.py.
    """
    def __init__(self, *args, **kwargs):

        # Call constructor of superclass (InterpolatedReplayBuffer)
        super().__init__(*args, **kwargs)

        # Overwrite default number of neighbors, since we only consider nearest
        self.kneighbors = 2

        # Disable use of queuing
        self.use_queue = False

    def interpolate_samples(self):
        """"Method for interpolating samples using nearest neighbors.

        This method performs interpolation by selecting samples through
        Importance Sampling and Prioritized Experience Replay. To form the
        interpolated_replay transition, this replay buffer takes the observation
        and action of the sampled point, and the reward and new obs or
        \delta obs of its nearest neighbor point. i.e. the interpolated_replay
        transition is constructed as:

        (sample_obs, sample_action, neighbor_reward, sample_obs + \delta_neighbor_obs)

        Returns:
            X_interp (np.array): A tuple composed of
                (obs, actions, rewards, next obs) of the training batch returned
                by the replay buffer.
            weights (None): N/A for this replay buffer. Set to None.
            interp_indices (list): Indices for the samples. If not needed,
                i.e. the interpolated priority update is not used, this is set
                to None.
            bs (np.array/list): An array of interpolation coefficients.
            neighbor_priorities (None): Indices for the neighbors. If not
                needed, i.e. the interpolated priority update is not used, this
                is set to None.
            sample_priorities (None): The priorities of the sample points. Only
                needed if the interpolated priority update is used, else can be
                set to None.
        """
        # Preprocess, sample points, and query neighbors
        self.sample_and_query()

        # Time loop
        time_start_interpolation = time.time()

        # Select closest neighbor for all neighbor sets in batch
        interp_indices = [k[1] for k in self.nearest_neighbors_indices]

        # Extract outputs from interpolated_replay x and predicted y
        Xn = self.X[self.sample_indices, :self.d_s]  # States
        An = self.X[self.sample_indices, self.d_s:]  # Actions
        Rn = np.squeeze(self.Y[interp_indices, :self.d_r])  # Rewards

        # Choose which interpolation task
        if self.use_delta:
            Xn1 = Xn + self.Y[interp_indices, self.d_r:]  # state + delta state
        else:
            Xn1 = self.Y[interp_indices, self.d_r:]  # next state only

        # Now transform array into SampleBatch with SampleBatchWrapper
        X_interpolated = (Xn, An, Rn, Xn1)

        # Set variables to None if they don't exist
        if not (self.weighted_updates and self.gaussian_process):
            weights = None  # Likelihood weights, not IS weights

        if not self.interp_prio_update:
            interp_indices = None
            b = None

        # Get neighbor weights, if we perform an interpolated_replay update
        if self.interp_prio_update:
            neighbor_priorities = self.replay_buffers[self.policy_id].priorities[interp_indices]
            sample_priorities = self.replay_buffers[self.policy_id].priorities[self.sample_indices]
        else:
            neighbor_priorities = None
            sample_priorities = None

        # Perform holdout evaluation to evaluate interpolation
        if self.perform_holdout_eval and self.step_count % self.holdout_freq == 0:
            self.perform_holdout_evaluation()

        # Stop timing interpolation
        time_end_interpolation = time.time()
        self.interpolation_time += time_end_interpolation - \
                                   time_start_interpolation
        self.average_interpolation_time = self.interpolation_time / self.step_count

        return X_interpolated, weights, interp_indices, b, \
            sample_priorities, neighbor_priorities

    def perform_holdout_evaluation(self):
        """Quantitative evaluation of interpolation quality.

        Method to perform holdout evaluation to assess the quality of
        interpolation. Though not an exact metric of interpolation quality for
        interpolated_replay samples, this method performs train/validation splits, and
        then assesses predictive performance on the holdout samples to determine
        interpolation accuracy of this KNN baseline in a batched setting.
        """
        # Get training and analytic_evaluation indices
        test_idx = np.random.randint(low=0, high=self.kneighbors,
                                     size=self.replay_batch_size)
        # Split the indices
        K_test = [k[test] for k, test in zip(self.nearest_neighbors_indices, test_idx)]
        Z_test_norm = np.array([self.X_norm[k_test] for k_test in K_test])
        Y_test = np.array([self.Y[k_test] for k_test in K_test])

        # Compute nearest neighbor indices - Faiss tree
        if self.use_faiss:
            pred_idx = self.faiss.query(Z_test_norm)[:, 1]

        # Compute nearest neighbor indices - KDTree
        elif self.use_kd_tree:
            pred_idx = self.kd_tree.query(Z_test_norm, k=2,
                return_distance=False, dualtree=True, sort_results=True)[:, 1]

        # Compute neighbors with KNN
        else:
            pred_idx = self.knn.kneighbors(Z_test_norm, return_distance=False)[:, 1]

        # Final predictions
        predicted_states = self.Y[pred_idx, self.d_r:]
        predicted_rewards = self.Y[pred_idx, :self.d_r]  # Squeezing not needed
        Y_test_hat = np.hstack((predicted_rewards, predicted_states))

        # Compute raw error across points and outputs
        E = np.subtract(Y_test_hat, Y_test)

        # Now compute error between predicted and true for each dimension
        holdout_rmse_by_dimension = np.mean(np.square(E), axis=0)
        holdout_mae_by_dimension = np.mean(np.abs(E), axis=0)

        # Log dimension-wise MSE
        for i, d_mse in enumerate(holdout_rmse_by_dimension):
            self.writer.add_scalar(
                "Holdout Evaluation-MSE/Average MSE, Dimension {}".format(i),
                d_mse, self.step_count)

        # Log dimension-wise MAE
        for i, d_mae in enumerate(holdout_mae_by_dimension):
            self.writer.add_scalar(
                "Holdout Evaluation-MAE/Average MAE, Dimension {}".format(i),
                d_mae, self.step_count)

        # Compute L1 and L2 norms of errors across dimensions
        holdout_l2_error = np.linalg.norm(E, ord=2, axis=1)
        holdout_l1_error = np.linalg.norm(E, ord=1, axis=1)

        # Compute average L1 and L2 norms of errors across batch
        holdout_avg_l2_error = np.mean(holdout_l2_error, axis=0)
        holdout_avg_l1_error = np.mean(holdout_l1_error, axis=0)

        # Consolidate metrics for tensorboard plotting
        metrics = [holdout_avg_l1_error, holdout_avg_l2_error]
        names = ["Holdout Evaluation-Correlation-Norm/Average L1 Error",
                 "Holdout Evaluation-Correlation-Norm/Average L2 Error"]

        # Loop over metrics and log to tb
        for name, metric in zip(names, metrics):
            # Log these quantities to tensorboard
            self.writer.add_scalar(name, metric, self.step_count)
            print("NAME: {}, METRIC: {}".format(name, metric))

        # Finally, plot PCA of predicted point relative to its true value
        if self.debug_plot:
            pca_holdout(Y_test_hat, Y_test, env=None)
