{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Matrix Numerical Stability\n",
    "To ensure that covariance matrices are not ill-conditioned, we can compute metrics on them that quantify their numerical stability. This is particularly important for covariance matrices because they must be positive semi-definite. Furthermore, when computing predictions for Gaussian Process Regression estimators, it is crucial we are able to find inverses or pseudo-inverses of these covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')  # Set style to make plots nicer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "Here, we'll create a dataset using NumPy `np.array` objects, and then will convert it to `torch.tensor` objects for use with PyTorch and GPyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "X = torch.tensor(np.arange(100)).float()\n",
    "\n",
    "# Create targets as noisy function of features\n",
    "Y = torch.tensor(np.add(np.sin(X / 10), np.random.normal(loc=0, scale=0.5, size=100))).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Here, we'll use the GPyTorch API to define a Gaussian Process Regression model. We'll use a `ConstantMean` function $m(x) = c$ for some $c \\in \\mathbb{R}$, and an outputscaled `MaternKernel` with discontinuity parameter $\\nu = \\frac{5}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(X, Y, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Loop\n",
    "Here, we'll use the `torch` backend of GPyTorch to run optimization using gradient descent methods, such as, in this case, `Adam`. Second-order optimization algorithms, such as `L-FBGS`, are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "training_iter = 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(X)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, Y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise-Free Predictions\n",
    "Here, we can predict the model-free predictive distribution at test points $f(\\mathbf{x}_{*})$. In the last line, we make predictions factoring in covariance noise ($\\sigma^{2}_n$), according to:\n",
    "$$y(\\mathbf{x}_{*}) = f(\\mathbf{x}_{*}) + \\epsilon, \\;\\;\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2}_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Places model in \"posterior mode\"\n",
    "model = model.eval()\n",
    "\n",
    "# Create test data\n",
    "X_test = torch.tensor(np.arange(0, 100, 0.01)).float()\n",
    "\n",
    "# Makes predictions without noise\n",
    "f_preds = model(X_test)\n",
    "\n",
    "# Attributes of predictive function\n",
    "f_mean = f_preds.mean\n",
    "f_var = f_preds.variance\n",
    "f_covar = f_preds.covariance_matrix\n",
    "\n",
    "# Compute observed predictions with likelihood model\n",
    "observed_pred = likelihood(model(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "We're now ready to make predictions with our model for $y(\\mathbf{x}_{*})$. Note the use of the `torch.no_grad()` context manager. The second context manager, `gpytorch.settings.fast_pred_var()`, allows for fast variance prediction using LOVE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.scatter(X.numpy(), Y.numpy())\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(X_test.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(X_test.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    plt.savefig(\"../../readme_images/gpr_ts_sine.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Metrics for Covariance Functions\n",
    "Here, we use the same covariance function as above to compute stability metrics for this covariance matrix, namely:\n",
    "\n",
    "1. The **condition number**. For the `p=2` norm, this is the ratio of the largest singular value to the smallest singular value: $$\\kappa(\\mathbf{K}_{XX}) = \\frac{\\sigma_{\\text{max}}(\\mathbf{K}_{XX})}{\\sigma_{\\text{min}}(\\mathbf{K}_{XX})}$$\n",
    "\n",
    "\n",
    "2. The **log determinant**. The less numerically stable the covariance matrix is, the more negative this log determinant will be (indicates that the eigenvalues of this matrix are approaching zero). This log determinant is approximated in `gpytorch` and `pytorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute Condition Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a random matrix (not necessarily positive definite)\n",
    "A = torch.rand(50, 50)  # Dimensions are (N_points, N_dimensions)\n",
    "\n",
    "# Compute the covariance for these points, and normalize by outputscale\n",
    "Kxx = model.covar_module(A) / model.covar_module.outputscale\n",
    "\n",
    "# Compute the condition number on the evaluated matrix\n",
    "condition_number = torch.linalg.cond(Kxx.evaluate(), p=2)\n",
    "print(\"Condition number is: {}\".format(float(condition_number)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute Log Determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option 1: Can compute using gpytorch and Lazytensors\n",
    "log_determinant_lazytensor = Kxx.logdet()\n",
    "print(\"Log determinant LazyTensors: {}\".format(log_determinant_lazytensor))\n",
    "\n",
    "# Option 2: Can evaluate the covariance matrix and compute using torch and Tensors\n",
    "log_determinant_evaluated = Kxx.evaluate().logdet()\n",
    "print(\"Log determinant Evaluated: {}\".format(log_determinant_evaluated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interreplay",
   "language": "python",
   "name": "interreplay"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
